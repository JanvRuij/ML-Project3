\section*{Exercise 1}
\subsection*{1}
For the generation of Quadratic knapsack instances the following has been applied:
\begin{enumerate}
	\item Items in total: 300
	\item Capacity of the knapsack: 15
	\item Weights of the items: Uniformly distributed from 0 to 1
	\item Value of the items: Uniformly distributed from 0 to 1
	\item Combinatory value of the items: Uniformly distributed from 0 to 1
\end{enumerate}
Solving this kind of instance with gurobi results in long runtimes (over 30 seconds). Setting the maximum execution time of the solver to 30 seconds will result in optimality gaps of around ~10\%.
\subsection*{2}
To explain how finding an optimal solution can take so long, one has to look at the amount of possible solutions. In this case, there are 300 items with an average weight of 0.5. Finding 30 items that will fit in the knapsack, is an easy task and calculating the end result as well. However, assuming the we have atleast 100 items with weight < 0.5. There will be atleast 30 items to put in the first position, 29 to put in the second postion etc. This will result in atleast $2.65 \times 10^{32}$ possible solutions. Therefore, calculating them all and comparing the results is very time consuming and as the amount of items increases, the amount of time needed to calculate increases in a non-polynomial matter. Having many possible solution is not really a problem when can you make use of derivatives to estimate where the optimal location may be. But in this case we have a binary decision variable and are unable to do so. CLIN does help bring some linearity into the original problem in exchange for more constraints. Overall, this does seem to help to help the gurobi solver to find a more optimal solution faster.
\subsection*{3}
\subsubsection*{a}
The neural network is going to decide which item will be in the knapsack. It will be trained on quadratic knapsack instances are described in section 1. However, the number of items will be decreased to 200 in total, the rest stays the same. This is due to the fact that we can find optimal sulutions in a faster manner this way (these are needed for training) and if we let the neural network decide which items will not be in the solution of an instance with 200 items, the items probably not be in an instance with 300 items. This is due to the fact that the capacity is equal, but there are only more items to choose from. The neural network looks at each item individually and recieves 5 pieces on information: 
\begin{enumerate}
	\item Value of the item
	\item Weight of the item
	\item 100th highest combinatory value of the item
	\item 99th highest combinatory value of the item
	\item 98th highest combinatory value of the item
\end{enumerate}
It was found in the training of the neural network that adding the top 100 highest combinary values was adding to much noise to the data. Adding the top combinatory values from 1 to 5, resulted in almost no improvement as well, this was due to the fact that when there are 300 items to have a good combinatory value with, all of the items had a very similar top 5. But as it turned out, adding the 98th, 99th and 100th item would bring better predictions from the network. 
\subsubsection*{b}
The overall design of the network looks as follows:
\begin{enumerate}
	\item Hidder layer 1: 20 nodes, relu activation
	\item Hidder layer 2: 20 nodes, relu activation
	\item Hidder layer 3: 4 nodes, relu activation
	\item Output layer 4: 1 node, sigmoid activation
\end{enumerate}
The model is optimized using adam's optimizer, with a learning rate of 0.003. In the dataset generated there are 20.000 items with their respective values, 15.000 of them were used to train the neural network and 5000 of them are used to test the neural network. The most important aspect to test the neural network on is the number of false negitaves. We want to bring better results, so removing an item that would have origianlly in the knapsack is a bad descision from the network. False positives do not really affect the end result at all, estimating an item as positive means the do not remove it (so do nothing).
\subsubsection*{c}
After training the neural network we en up with threshhold result, these results shows the amount of True/False positves and True/False negatives, an overview of these results can be found in the APPENDIX. The over accuracy of the neural network is 0.97. To compare the mathheuristic to the original CLIN, we have done 10 testruns where both the original CLIN and the mathheuristic have a time limit of 30 seconds, and runs where the mathheuristic has a time limit of 25 seconds. This is mainly due to the fact that the mathheuristic is already spending time deciding whether or not we should include an item in the solver or not. The overall results can be found in APPENDIX2 APPENDIX3. The mathheuristic does completely outperform the original problem.
